{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title of the Journal...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [**Libraries and Preliminaries**](#Libraries-and-Preliminaries)\n",
    "2. [**Image Preprocessing**](#Image-Preprocessing)\n",
    "    1. [Normalizing and Chipping Sentinel-2 Images](#Normalize-the-Sentinel-2-Images)\n",
    "    2. [Creating a Dataset of Chips](#Creating-a-Dataset-of-Chips)\n",
    "3. [**Deep Learning - ResNet-50**](#Deep-Learning)\n",
    "    1. [Passing Normalized Chips through the Network](#Passing-Normalized-Chips-through-the-Network)\n",
    "    2. [Extracting ResNet-50 Features](#Extracting-ResNet-50-Features)\n",
    "4. [**Features and a First Statistical Analysis**](#Features-and-a-First-Statistical-Analysis)\n",
    "    1. [Feature Histograms](#Created-Histograms-for-the-2,048-Features-and-Saved-them-in-a-File)\n",
    "    2. [Conducting KL-Divergence for all the Histograms Created](#Conducting-KL-Divergence-for-all-the-Histograms-Created)\n",
    "5. [**Chip and Census Tract Polygons**](#Chip-and-Census-Tract-Polygons)\n",
    "    1. [Extracting Longitude and Latitude from the jp2 Image](#Extracting-Longitude-and-Latitude-from-the-jp2-Image)\n",
    "    2. [Joining the Census Tract Polygon and the Chips Polygon Tables](#Joining-the-Census-Tract-Polygon-and-the-Chips-Polygon-Tables)\n",
    "    3. [Show the Census Tract Polygon (in Blue) and the Chips Polygon (in Green)](#Show-the-Census-Tract-Polygon-(in-Blue)-and-the-Chips-Polygon-(in-Green))\n",
    "6. [**Machine Learning Analysis**](#Machine-Learning-Analysis)\n",
    "    1. [Linear Regression](#Linear-Regression)\n",
    "        1. [Linear Regression for all Features](#Linear-Regression-for-all-Features)\n",
    "        2. [Linear Regression Scatter Plot for Testing](#Linear-Regression-Scatter-Plot-for-Testing)\n",
    "        3. [Linear Regression for Scatter Plot for Training](#Linear-Regression-for-Scatter-Plot-for-Training)\n",
    "    2. [Random Forest](#Random-Forest)\n",
    "        1. [Random Forest for All Features](#Random-Forest-for-All-Features)\n",
    "        2. [Random Forest Scatter Plot](#Random-Forest-Scatter-Plot)\n",
    "        3. [Renamed the df to df_final](#Renamed-the-df-to-df_final)\n",
    "    3. [Averaging Feature Vectors](#Averaging-Feature-Vectors)\n",
    "        1. [Linear Regression for the Average Features Vectors for Each Census Tract](#Linear-Regression-for-the-Average-Features-Vectors-for-Each-Census-Tract)\n",
    "        2. [Linear Regression for the Area Weighted Average for Each Census Tract](#Linear-Regression-for-the-Area-Weighted-Average-for-Each-Census-Tract)\n",
    "        3. [The Average Features Vectors for Each Census Tract](#The-Average-Features-Vectors-for-Each-Census-Tract)\n",
    "        4. [Linear Regression for the Average Features Vectors for Each Census Tract (All Features)](#Linear-Regression-for-the-Average-Features-Vectors-for-Each-Census-Tract-(All-Features))\n",
    "        5. [Linear Regression for the Average Features Vectors for Each Census Tract (Selected Features)](#Linear-Regression-for-the-Average-Features-Vectors-for-Each-Census-Tract-(Selected-Features))    \n",
    "    4. [Selecting \"Best\" Features](#Selecting-\"Best\"-Features)\n",
    "        1. [Random Selection for the Average Features Vectors for Each Census Tract (All Features)](#Random-Selection-for-the-Average-Features-Vectors-for-Each-Census-Tract-(All-Features))\n",
    "        2. [Random Selection for the Average Features Vectors for Each Census Tract (Selected Features)](#Random-Selection-for-the-Average-Features-Vectors-for-Each-Census-Tract-(Selected-Features))\n",
    "        3. [The Area Weighted Average for Each Census Tract](#The-Area-Weighted-Average-for-Each-Census-Tract)\n",
    "    5. [Regression with Best Weighted Features](#Regression-with-Best-Weighted-Features)\n",
    "        1. [Linear Regression for the Area Weighted Average for Each Census Tract](#Linear-Regression-for-the-Area-Weighted-Average-for-Each-Census-Tract)\n",
    "        2. [Random Forest for the Area Weighted Average for Each Census Tract](#Random-Forest-for-the-Area-Weighted-Average-for-Each-Census-Tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Libraries and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.io as sio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv('DATA_DIR') #set as environment variable\n",
    "if DATA_DIR is None:DATA_DIR = './data'\n",
    "CHIP_DIR  = DATA_DIR + '/chips'\n",
    "CHIP_SIZE = 224\n",
    "\n",
    "blue_images  = sorted(glob.glob(DATA_DIR + '/*/T*B02*.jp2'))  # blue in any .SAFE dir\n",
    "green_images = sorted(glob.glob(DATA_DIR + '/*/T*B03*.jp2'))  # green in any .SAFE dir\n",
    "red_images   = sorted(glob.glob(DATA_DIR + '/*/T*B04*.jp2'))  # red in any .SAFE dir\n",
    "all_bands    = blue_images + green_images + red_images\n",
    "N            = len(all_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2 - Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A - Normalizing and Chipping Sentinel-2 Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Worker Function for Each Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# FUNCTION Def\n",
    "#--------------------------------------------------\n",
    "def chip_and_normalize(input_path,index):\n",
    "    #some feedback\n",
    "    filename = input_path.split(\"/\")[-1]\n",
    "    print(f'[{index}/{N}] Processing {filename}')\n",
    "    \n",
    "    #input variables\n",
    "    src        = rio.open(input_path,'r')\n",
    "    raster_max = src.statistics(1).max\n",
    "    raster_min = src.statistics(1).min\n",
    "    basename   = os.path.basename(input_path).split('.')[0]\n",
    "\n",
    "    #output variables\n",
    "    kwargs     = src.meta.copy()\n",
    "    kwargs.update({'driver':'GTiff','height':CHIP_SIZE,'width':CHIP_SIZE,'dtype':np.float32})\n",
    "    \n",
    "    #Loop thru chips (by i row, j column)\n",
    "    for i,x in enumerate(range(0,src.height-CHIP_SIZE,CHIP_SIZE)):\n",
    "        for j,y in enumerate(range(0,src.width-CHIP_SIZE,CHIP_SIZE)):\n",
    "\n",
    "            #output path\n",
    "            output_path = f\"{CHIP_DIR}/{basename}_CHIP_{i:02d}_{j:02d}.tif\"\n",
    "\n",
    "            #Read window from input and normalize\n",
    "            win  = rio.windows.Window(col_off=y,row_off=x,width=CHIP_SIZE,height=CHIP_SIZE)\n",
    "            chip = src.read(1,window=win)\n",
    "            chip = (chip - raster_min)/(raster_max - raster_min) #Normalize to [0,1]\n",
    "            # chip = (chip * 65535).astype(int) #Stretch to [0,65535]\n",
    "\n",
    "            #write to output file\n",
    "            with rio.open(output_path, 'w',**kwargs) as dst:\n",
    "                dst.write(chip,indexes=1)\n",
    "\n",
    "    #signal done and close\n",
    "    print(f'{filename} done.')\n",
    "    src.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Each Thread a Copy of the Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Processing T15SVC_20220720T165901_B02_10m.jp2\n",
      "[2/6] Processing T16SCG_20220813T163911_B02_10m.jp2\n",
      "[3/6] Processing T15SVC_20220720T165901_B03_10m.jp2\n",
      "[4/6] Processing T16SCG_20220813T163911_B03_10m.jp2\n",
      "[5/6] Processing T15SVC_20220720T165901_B04_10m.jp2\n",
      "[6/6] Processing T16SCG_20220813T163911_B04_10m.jp2\n",
      "T16SCG_20220813T163911_B02_10m.jp2 done.\n",
      "T15SVC_20220720T165901_B03_10m.jp2 done.\n",
      "T16SCG_20220813T163911_B03_10m.jp2 done.\n",
      "T15SVC_20220720T165901_B02_10m.jp2 done.\n",
      "T15SVC_20220720T165901_B04_10m.jp2 done.\n",
      "T16SCG_20220813T163911_B04_10m.jp2 done.\n",
      "Execution time: 26.2496 seconds.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "# MULTI-THREADED\n",
    "#--------------------------------------------------\n",
    "if not os.path.isdir(CHIP_DIR):\n",
    "    os.mkdir(CHIP_DIR)\n",
    "\n",
    "if os.cpu_count() > 16:\n",
    "    n_workers = 16 - 1\n",
    "else:\n",
    "    n_workers = os.cpu_count() - 1 \n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=n_workers) as executor: \n",
    "    # for idx,raster_path in enumerate(all_bands):\n",
    "        # executor.submit(chip_and_normalize,raster_path,CHIP_DIR,idx)\n",
    "    executor.map(chip_and_normalize,all_bands,range(1,N+1))\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {round(end_time-start_time,4)} seconds.\")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# SINGLE-THREADED\n",
    "#--------------------------------------------------\n",
    "# start_time = time.time()\n",
    "# for i,raster_path in enumerate(all_bands):\n",
    "    # print(f\"[{i+1}/{len(all_bands)}] \",end='')\n",
    "#     chip_and_normalize(raster_path,CHIP_DIR)\n",
    "# end_time = time.time()\n",
    "# print(f\"Execution time: {round(end_time-start_time,4)} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize an Example Chip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_paths = sorted(glob.glob(CHIP_DIR + '/T*B02*.tif')) #--- paths, len is 2401 * nr.of images\n",
    "sample_b2_path = b2_paths[1500]\n",
    "sample_b3_path = sample_b2_path.replace('_B02_','_B03_')\n",
    "sample_b4_path = sample_b2_path.replace('_B02_','_B04_')\n",
    "\n",
    "sample_b = sio.imread(sample_b2_path) #--- read images and stack\n",
    "sample_g = sio.imread(sample_b3_path)\n",
    "sample_r = sio.imread(sample_b4_path)\n",
    "rgb      = np.stack([sample_r,sample_g,sample_b],axis=2)\n",
    "\n",
    "normalize = lambda x: (x-x.min())/(x.max() - x.min()) #for plot brightness\n",
    "plt.imsave('./figs/sample_chip.png',normalize(rgb))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3 - Deep Learning - Extracting ResNet-50 Features for Each Chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET DEVICE\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') #mac\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataset of Chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentinelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.images_b = sio.imread_collection(f'{CHIP_DIR}/*_B02_10m_CHIP_*_*.tif')\n",
    "        # self.images_g = sio.imread_collection(f'{CHIP_DIR}/*_B03_10m_CHIP_*_*.tif')\n",
    "        # self.images_r = sio.imread_collection(f'{CHIP_DIR}/*_B04_10m_CHIP_*_*.tif')\n",
    "        self.images_b = sorted(glob.glob(f'{CHIP_DIR}/*_B02_10m_CHIP_*_*.tif'))\n",
    "        self.images_g = sorted(glob.glob(f'{CHIP_DIR}/*_B03_10m_CHIP_*_*.tif'))\n",
    "        self.images_r = sorted(glob.glob(f'{CHIP_DIR}/*_B04_10m_CHIP_*_*.tif'))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        b = sio.imread(self.images_b[index])\n",
    "        g = sio.imread(self.images_g[index])\n",
    "        r = sio.imread(self.images_r[index])\n",
    "        rgb_img = torch.from_numpy(np.stack([r,g,b], axis=0))\n",
    "        return rgb_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_b)\n",
    "\n",
    "\n",
    "dataset    = SentinelDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Neural Network and Removing its Top Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained = True,trust_repo=True,verbose=True,skip_validation=True)\n",
    "resnet50.fc = torch.nn.Identity() # Remove final classification layer\n",
    "resnet50 = resnet50.eval()\n",
    "# resnet50 = resnet50.to(device)\n",
    "# -------\n",
    "# In case above model does not load, use torchvision resnet:\n",
    "# resnet50 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "# resnet50.fc = torch.nn.Identity() # Remove final classification layer\n",
    "# resnet50 = resnet50.eval()\n",
    "# resnet50 = resnet50.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Normalized Chips through the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_CSV  = f'{DATA_DIR}/features.csv'\n",
    "features_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _,x in tqdm.tqdm(enumerate(dataloader), total=(len(dataset))):\n",
    "        x = x.to(device)\n",
    "        y = resnet50(x)\n",
    "        y = y.cpu().numpy()\n",
    "        features_list.append(y)\n",
    "\n",
    "stop_time  = time.time()\n",
    "execution_time = stop_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "#SAVE FEATURES TO DISK\n",
    "np.savetxt(FEATURES_CSV,np.array(features_list),delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Statistical Analysis for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.concatenate(features_list)\n",
    "df_features = pd.DataFrame(all_features)\n",
    "df_features.columns = [f'feature_{i}' for i in range(2048)]\n",
    "df_features.index = dataset.images_r.files\n",
    "df_features.to_csv(FEATURES_CSV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(FEATURES_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = df_features['feature_0'].mean()\n",
    "\n",
    "print(\"Mean of 'feature_0':\", mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = df_features['feature_0'].median()\n",
    "\n",
    "print(\"Median of 'feature_0':\", median_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_value = df_features['feature_0'].mode()\n",
    "\n",
    "print(\"Mode of 'feature_0':\", mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = df_features['feature_0'].min()\n",
    "max_value = df_features['feature_0'].max()\n",
    "data_range = max_value - min_value\n",
    "\n",
    "print(f\"Minimum value of 'feature_0': {min_value}\")\n",
    "print(f\"Maximum value of 'feature_0': {max_value}\")\n",
    "print(f\"Range of {data_range}: {data_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df_features' is our DataFrame with 2048 columns\n",
    "\n",
    "# Calculate minimum for each column\n",
    "min_values = np.min(df_features, axis=0)\n",
    "\n",
    "# Calculate maximum for each column\n",
    "max_values = np.max(df_features, axis=0)\n",
    "\n",
    "# Calculate range (max - min) for each column\n",
    "# range_values = np.ptp(df_features, axis=0)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "summary_df_features = pd.DataFrame({\n",
    "    'Min': min_values,\n",
    "    'Max': max_values,\n",
    "    #'Range': range_values\n",
    "})\n",
    "\n",
    "# Print the summary DataFrame\n",
    "print(summary_df_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis for Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created Histograms for the 2,048 Features and Saved them in a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# bin_width = 0.05\n",
    "\n",
    "# for i in range(2048):\n",
    "#     feature_name = f'feature_{i}'\n",
    "    \n",
    "#     # Generate histogram\n",
    "#     hist, bin_edges = np.histogram(df_features[feature_name], bins=int(1 / bin_width), range=(0, 1), density=True)\n",
    "    \n",
    "#     # Plot the histogram\n",
    "#     plt.bar(bin_edges[:-1], hist, width=bin_width)\n",
    "    \n",
    "#     plt.title(f'Histogram of {feature_name}')\n",
    "#     plt.xlabel('Value')\n",
    "#     plt.ylabel('Frequency')\n",
    "\n",
    "#     # Define a file name to save the histogram\n",
    "#     file_name = f'histogram_{i}.png'\n",
    "    \n",
    "#     if i < 3:\n",
    "#         plt.show() \n",
    "\n",
    "#     # Save the histogram to a file (e.g., as a PNG image)\n",
    "#     plt.savefig(f'histograms/histogram_{feature_name}.png')\n",
    "    \n",
    "#     # Clear the current plot for the next iteration\n",
    "#     plt.clf()\n",
    "\n",
    "#     print(f'Saved histogram for {feature_name} as {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#bin_width = 0.05\n",
    "\n",
    "#for i in range(2048):\n",
    "    #feature_name = f'feature_{i+1}'  # Assuming feature names are like feature_1, feature_2, ...\n",
    "    \n",
    "    # Calculate the histogram\n",
    "    #hist, edges = np.histogram(df_features[feature_name], bins=int(1 / bin_width), range=(0, 1), density=True)\n",
    "    \n",
    "    # Plot the histogram\n",
    "    #plt.bar(edges[:-1], hist, width=bin_width)\n",
    "    \n",
    "    #plt.title(f'Histogram of {feature_name}')\n",
    "    #plt.xlabel('Value')\n",
    "    #plt.ylabel('Frequency')\n",
    "    \n",
    "    # Define a file name to save the histogram\n",
    "    #file_name = f'histogram_{i+1}.png'\n",
    "    \n",
    "    # Save the histogram to a file\n",
    "    #plt.savefig(file_name)\n",
    "    \n",
    "    # Clear the current plot for the next iteration\n",
    "    #plt.clf()\n",
    "\n",
    "    #print(f'Saved histogram for {feature_name} as {file_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting KL-Divergence for all the Histograms Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "# Calculate KL divergences\n",
    "kl_divergences = []\n",
    "\n",
    "for i in tqdm.tqdm(range(2048)):\n",
    "    for j in range(i+1, 2048):\n",
    "        \n",
    "        hist_i, _ = np.histogram(df_features[f'feature_{i}'], bins=np.linspace(0, 5, 500))\n",
    "        hist_j, _ = np.histogram(df_features[f'feature_{j}'], bins=np.linspace(0, 5, 500))\n",
    "\n",
    "        kl_value = kl_divergence(hist_i, hist_j)\n",
    "        \n",
    "        kl_divergences.append((i, j, kl_value))\n",
    "\n",
    "# Sort the list of KL divergences\n",
    "kl_divergences.sort(key=lambda x: x[2])\n",
    "\n",
    "# Print the top 10 pairs with the smallest KL divergences\n",
    "for i, j, kl_value in kl_divergences[:10]:\n",
    "    print(f'KL Divergence between feature_{i} and feature_{j}: {kl_value}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip and Census Tract Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Longitude and Latitude from the jp2 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.columns = ['file', *df_features.columns[1:]]\n",
    "def get_original_jp2(chip):\n",
    "    return chip.replace('/home/jovyan/output_images_all/', '/data/obesity-images/all/').split('_normalized')[0] + '.jp2'\n",
    "df_features['jp2'] = df_features['file'].apply(get_original_jp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "import functools\n",
    "\n",
    "@functools.lru_cache(None)\n",
    "def get_coords(jp2_file_path):\n",
    "    # jp2_file_path = '/data/JAMIA/T15SWD_20230829T165849_B02_10m.jp2'\n",
    "    with rasterio.open(jp2_file_path) as dataset:\n",
    "        # Assuming the image is georeferenced, this will get the bounds\n",
    "        bounds = dataset.bounds\n",
    "        x1, y1, x2, y2 = bounds.left, bounds.top, bounds.right, bounds.bottom\n",
    "        w, h = dataset.width, dataset.height\n",
    "    return x1, y1, x2, y2, w, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, x2, y2, w, h = get_coords('/data/JAMIA/T15SWD_20230829T165849_B02_10m.jp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['image_coords'] = df_features['jp2'].apply(get_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln /data/obesity-images/*/*.jp2 /data/obesity-images/all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Define transformer to convert from UTM zone 15N to WGS 84\n",
    "transformer = Transformer.from_crs(dataset.crs, \"epsg:4326\", always_xy=True)\n",
    "\n",
    "# jp2_file_path = '/data/JAMIA/T15SWD_20230829T165849_B02_10m.jp2'\n",
    "\n",
    "# with rasterio.open(jp2_file_path) as dataset:\n",
    "#     # Assuming the image is georeferenced, this will get the bounds\n",
    "#     bounds = dataset.bounds\n",
    "#     x1, y1, x2, y2 = bounds.left, bounds.top, bounds.right, bounds.bottom\n",
    "#     w, h = dataset.width, dataset.height\n",
    "\n",
    "\n",
    "# Modify the get_lat_lon function to return a Polygon\n",
    "def get_lat_lon(f):\n",
    "    jp2 = get_original_jp2(f)\n",
    "    x1, y1, x2, y2, w, h = get_coords(jp2)\n",
    "\n",
    "    resolution_x = (x2 - x1) / w\n",
    "    resolution_y = (y2 - y1) / h\n",
    "\n",
    "    x, y = f[-15:-4].split('_')\n",
    "    x, y = int(x), int(y)\n",
    "    lon1 = x1 + resolution_x * x\n",
    "    lat1 = y1 + resolution_y * y\n",
    "    lon2 = lon1 + 224 * resolution_x\n",
    "    lat2 = lat1 + 224 * resolution_y\n",
    "    \n",
    "    # Transform the coordinates\n",
    "    lon1, lat1 = transformer.transform(lon1, lat1)\n",
    "    lon2, lat2 = transformer.transform(lon2, lat2)\n",
    "\n",
    "    # Create a Polygon object\n",
    "    return Polygon([(lon1, lat1), (lon2, lat1), (lon2, lat2), (lon1, lat2)])\n",
    "\n",
    "# Apply the function to the 'file' column\n",
    "df_features['geometry'] = df_features.iloc[:, 0].apply(get_lat_lon, )\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df_features, geometry='geometry')\n",
    "\n",
    "gdf.to_csv('chips-features-polygons.csv')\n",
    "\n",
    "# Example usage\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.read_csv('chips-features-polygons.csv')\n",
    "\n",
    "# Example usage\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "\n",
    "obesity_df = pd.read_csv('final-original.csv')\n",
    "obesity_df.index = obesity_df['Unnamed: 0']\n",
    "obesity_df = obesity_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "def maybe_wkt_load(inp):\n",
    "    try:\n",
    "        return wkt.loads(inp)\n",
    "    except Exception:\n",
    "        # print('failed')\n",
    "        return None\n",
    "# obesity_df['geometry'] = obesity_df['geometry'].apply(maybe_wkt_load)\n",
    "obesity_df['geometry'] = obesity_df['geometry'].apply(wkt.loads) \n",
    "obesity_gdf = gpd.GeoDataFrame(obesity_df, crs='epsg:4326') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf = obesity_gdf[obesity_gdf['geometry'].apply(lambda x: x is not None)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features per chip polygon\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf_4 = obesity_gdf.to_crs(epsg='26915')\n",
    "obesity_gdf_4[\"area\"] = obesity_gdf_4[\"geometry\"].area / 1000000\n",
    "obesity_gdf_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf_4.reset_index(inplace=True)\n",
    "obesity_gdf_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obesity_gdf_5 = obesity_gdf_4[obesity_gdf_4[\"CountyName\"] == \"Boone\"]\n",
    "# obesity_gdf_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_area = obesity_gdf_4[[\"GEOID\", \"TotalPopulation\", \"area\", \"OBESITY_CrudePrev\"]]\n",
    "#obs_area = obs_area[obs_area[\"CountyName\"] == \"Boone\"]\n",
    "obs_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obesity rates per census tract polygon\n",
    "\n",
    "obesity_gdf[['OBESITY_CrudePrev', 'geometry', 'GEOID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obesity rates per census tract polygon\n",
    "\n",
    "obesity_gdf[['OBESITY_CrudePrev', 'geometry', 'GEOID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obesity_gdf.plot('GEOID')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "obesity_gdf.plot(column='OBESITY_CrudePrev', ax=ax, legend=True)\n",
    "\n",
    "ax.set_xlabel('Polygon Geometry')\n",
    "ax.set_ylabel('Obesity Rate') \n",
    "\n",
    "ax.set_title('Obesity Rates per Census Tract Polygon')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "obesity_gdf.plot(column='OBESITY_CrudePrev', ax=ax, legend=True, cmap=\"Reds\")\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# legend = ax.get_legend()\n",
    "# legend.set_title('Obesity per 100k')\n",
    "# ax.set_title(label='Obesity Rate per 100k')\n",
    "\n",
    "# ax.set_title('Obesity Rates per Census Tract Polygon') # Census Tract-wise Obesity Rates in Missouri \n",
    "ax.set_title('Census Tract-wise Obesity Rates in Missouri (2022)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "obesity_gdf.plot(column='OBESITY_CrudePrev', ax=ax, legend=True, cmap=\"Reds\")\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# legend = ax.get_legend()\n",
    "# legend.set_title('Obesity per 100k')\n",
    "# ax.set_title(label='Obesity Rate per 100k')\n",
    "\n",
    "# ax.set_title('Obesity Rates per Census Tract Polygon') # Census Tract-wise Obesity Rates in Missouri \n",
    "# ax.set_title('Census Tract-wise Obesity Rates in Missouri (2022)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7,3.5))\n",
    "obesity_gdf.plot(ax=ax[0], legend=True, cmap=\"Reds\")\n",
    "obesity_gdf.plot(ax=ax[1], column='OBESITY_CrudePrev', legend=True, cmap=\"Reds\")\n",
    "\n",
    "# ax.set_xlabel('Longitude')\n",
    "# ax.set_ylabel('Latitude')\n",
    "\n",
    "# ax.set_title('Obesity Rates per Census Tract Polygon')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining the Census Tract Polygon and the Chips Polygon Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left spatial join\n",
    "\n",
    "obesity_gdf['OBESITY_CrudePrev'] = obesity_gdf['OBESITY_CrudePrev'].apply(float)\n",
    "joined_gdf = gpd.sjoin(gdf, obesity_gdf[['OBESITY_CrudePrev', 'geometry', 'GEOID']], how='left', predicate='intersects',)\n",
    "# joined_gdf[['file', 'OBESITY_CrudePrev']].groupby('file').mean()\n",
    "\n",
    "joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the joined gdf as csv file\n",
    "\n",
    "joined_gdf.to_csv('all_joined_gdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf = pd.read_csv('all_joined_gdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "obesity_gdf.plot(alpha=0.1, ax=ax)\n",
    "gdf.plot(ax=ax, facecolor='green')\n",
    "\n",
    "ax.set_xlabel('Longitude') \n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "ax.set_title('Sentinel-2 Image within Missouri')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ax.equal_axis(True)\n",
    "# plt.ylim(39.6, 39.7)\n",
    "# plt.xlim(-93.0, -92.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf = joined_gdf.sort_values('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf[[\n",
    "    'GEOID', 'index_right'\n",
    "]].groupby('GEOID').count().to_csv('num_chips_per_ct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_gdf.groupby('GEOID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chip with the most intersections\n",
    "joined_gdf_1 = joined_gdf[joined_gdf['file'] == '/home/jovyan/output_images_all/T15SUD_20220710T165911_B04_10m_normalized_chip_06272_07392.tif']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "obesity_gdf.loc[joined_gdf_1['index_right'].values].plot('OBESITY_CrudePrev', cmap='jet', ax=ax, legend=True)\n",
    "joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='w')\n",
    "\n",
    "ax.set_title(f'Average {joined_gdf_1[\"OBESITY_CrudePrev\"].mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 30 # 608\n",
    "joined_gdf_1 = joined_gdf[joined_gdf['index_right'] == IDX]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "obesity_gdf[obesity_gdf.index == IDX].plot(ax=ax, legend=True, facecolor='none', edgecolor='b')\n",
    "joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='g')\n",
    "ax.set_title(f'Average {joined_gdf_1[\"OBESITY_CrudePrev\"].mean():.2f}')\n",
    "\n",
    "ax.set_xlabel('Longitude') \n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "ax.set_title(f'Number of Image Chips Intersecting with Census Tract ({joined_gdf_1.shape[0]} chips)' )\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf_1 = joined_gdf[\n",
    "    joined_gdf['file'] == '/home/jovyan/output_images_all/T15SUD_20220710T165911_B04_10m_normalized_chip_06272_07392.tif']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "obesity_gdf_1 = obesity_gdf.loc[joined_gdf_1['index_right'].values]\n",
    "obesity_gdf_1[\"GEOID\"] = obesity_gdf_1[\"GEOID\"].astype('str') \n",
    "obesity_gdf_1[\"tractID\"] = obesity_gdf_1[\"GEOID\"].str[-6:-2] #Change the range whatever you want \n",
    "\n",
    "obesity_gdf_1.plot(ax=ax, legend=True, facecolor='none', edgecolor='b')\n",
    "joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='g')\n",
    "ax.set_title(f'Average {joined_gdf_1[\"OBESITY_CrudePrev\"].mean():.2f}')\n",
    "\n",
    "for idx, row in obesity_gdf_1.iterrows():\n",
    "    # Assuming the geometries are Point. For other types, you might need to adjust this.\n",
    "    if row.geometry.type == 'Point':\n",
    "        plt.annotate(row['GEOID'], xy=(row.geometry.x, row.geometry.y),\n",
    "                     horizontalalignment='center')\n",
    "    # For Polygon geometries, you might use the centroid or another representative point\n",
    "    elif row.geometry.type == 'Polygon':\n",
    "        plt.annotate(row['tractID'], xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n",
    "                     horizontalalignment='center')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Longitude') \n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_gdf_1[\"GEOID\"] = obesity_gdf_1[\"GEOID\"].astype('str') \n",
    "obesity_gdf_1[\"tractID\"] = obesity_gdf_1[\"GEOID\"].str[-6:-2]\n",
    "#obesity_gdf_1[\"area\"] = obesity_gdf_1[\"geometry\"].area\n",
    "obesity_gdf_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obesity_gdf_2 = obesity_gdf_1.to_crs(epsg='26915').reset_index()\n",
    "# obesity_gdf_2[\"area\"] = obesity_gdf_2[\"geometry\"].area/1000000\n",
    "# obesity_gdf_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_area = obesity_gdf_2[[\"GEOID\", \"TotalPopulation\", \"area\", \"OBESITY_CrudePrev\"]]\n",
    "# obs_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chip with the most intersections\n",
    "# joined_gdf_1 = joined_gdf[joined_gdf['Unnamed: 0']]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "gdf.iloc[::5].plot(ax=ax, legend=True)\n",
    "# joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='w')\n",
    "\n",
    "ax.set_title(f'Average {joined_gdf_1[\"OBESITY_CrudePrev\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Census Tract Polygon (in Blue) and the Chips Polygon (in Green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chip = gdf['Unnamed: 0'][0]\n",
    "def getxy(chip):\n",
    "    x, y = chip[-15:-4].split('_')\n",
    "    x, y = int(x), int(y)\n",
    "    return x, y\n",
    "\n",
    "def getx(chip):\n",
    "    return getxy(chip)[0]\n",
    "\n",
    "def gety(chip):\n",
    "    return getxy(chip)[1]\n",
    "\n",
    "gdf['x'] = gdf['file'].apply(getx)\n",
    "gdf['y'] = gdf['file'].apply(gety)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_10 = gdf[(gdf['x'] % 1 == 0) * (gdf['y'] % 1 == 0)]\n",
    "# gdf_10.plot()\n",
    "\n",
    "# for tile in :\n",
    "joined_gdf_1 = joined_gdf[joined_gdf['file'].isin(gdf_10['file'].values)]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "x1, x2 = -92.4, -92\n",
    "y1, y2 = 38.85, 39.25\n",
    "# ax.set_xlim(x1, x2)\n",
    "# ax.set_ylim(y1, y2)\n",
    "joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='g')\n",
    "obesity_gdf.loc[joined_gdf_1['index_right'].dropna().values].plot(\n",
    "    ax=ax, legend=True, facecolor='none', edgecolor='b')\n",
    "ax.plot([x1, x2, x2, x1, x1], [y2, y2, y1, y1, y2], c='r')\n",
    "# ax.set_aspect('equal', adjustable='box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_10 = gdf[(gdf['x'] < 2500) * (gdf['y'] < 2500)]\n",
    "# gdf_10.plot()\n",
    "\n",
    "# for tile in :\n",
    "joined_gdf_1 = joined_gdf[joined_gdf['file'].isin(gdf_10['file'].values)]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "obesity_gdf.loc[joined_gdf_1['index_right'].dropna().values].plot(\n",
    "    ax=ax, legend=True, facecolor='none', edgecolor='b')\n",
    "joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf_1['index_right'].dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_10 = gdf#[(gdf['x'] < 2500) * (gdf['y'] < 2500)]\n",
    "# gdf_10.plot()\n",
    "\n",
    "# for tile in :\n",
    "joined_gdf_1 = joined_gdf[joined_gdf['file'].isin(gdf_10['file'].values)]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "obesity_gdf.loc[joined_gdf_1['index_right'].dropna().values].plot(\n",
    "    ax=ax, legend=True, facecolor='none', edgecolor='b')\n",
    "joined_gdf_1.plot(ax=ax, facecolor='none', edgecolor='g')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Longitude') \n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "ax.set_title('Image Chips Intersecting with each Census Tract')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.dropna().to_csv('Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for all Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'all_joined_gdf.csv'  # Replace it with our file path\n",
    "data = pd.read_csv(file_path).dropna()\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "# selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = LinearRegression()\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation\n",
    "data.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Scatter Plot for Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')\n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for Scatter Plot for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_sel = model_sel.predict(X_train_sel)\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for All Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'joined_gdf.csv'  # Replace it with our file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "# selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = RandomForestRegressor(n_jobs=-1, verbose=2)\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting feature importances from the model\n",
    "feature_importances = model_sel.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for better visualization\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X_selected.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame based on importance\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Displaying the feature importances\n",
    "features_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[:10].plot.bar(y='Importance') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting feature importances from the model\n",
    "feature_importances = model_sel.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for better visualization\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X_selected.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame based on importance\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Displaying the feature importances\n",
    "features_df.tail(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[-10:].plot.bar(x='Feature', y='Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "gdf = gpd.GeoDataFrame(data, geometry=data['geometry'].apply(loads)) \n",
    "# gdf.plot(column=\"feature_1112\");\n",
    "gdf.plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Scatter Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename and save the df to df_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'all_joined_gdf.csv'  # Replace it with the path to our CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.to_csv('df_final.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Image Chips that Intersect with Each Census Tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_counts = df.drop(['geometry', 'Unnamed: 0'], axis=1).groupby('GEOID').count()\n",
    "df_counts['file'].hist()\n",
    "plt.xlabel('Number of chips per census tract')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Average Features Vectors for Each Census Tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_stats = pd.read_csv('/data/JAMIA/JAMIA_Obs_CT_final.csv', encoding='utf-8')\n",
    "ct_stats[['polygon_nr','CountyName','TotalPopulation','OBESITY_CrudePrev','TractFIPS','GEOID','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.wkt import loads\n",
    "\n",
    "# ct_stats = pd.read_csv('/data/JAMIA/obs_area_pop_final.csv')\n",
    "# ct_stats = pd.read_csv('/data/JAMIA/JAMIA_Obs_CT_final.csv')\n",
    "# ct_stats.index = ct_stats['GEOID']\n",
    "\n",
    "ct_stats = obesity_gdf\n",
    "\n",
    "ct_stats = ct_stats.to_crs(epsg='26915').reset_index()\n",
    "ct_stats[\"area\"] = ct_stats[\"geometry\"].area/1000000\n",
    "ct_stats.head()\n",
    "\n",
    "# Convert the 'geometry' column from WKT format to Shapely geometries\n",
    "# ct_stats = gpd.GeoDataFrame(ct_stats, geometry=df['geometry'].apply(loads))\n",
    "# Calculate the area for each row\n",
    "# ct_stats['area'] = ct_stats['geometry'].area\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "ct_stats['TotalPopulation'].hist(ax=axs[0])\n",
    "ct_stats['area'].hist(ax=axs[1])\n",
    "ct_stats['OBESITY_CrudePrev'].hist(ax=axs[2])\n",
    "\n",
    "# titles\n",
    "axs[0].set_title('Total population')\n",
    "axs[1].set_title('Area')\n",
    "axs[2].set_title('Obesity Rate [%]')\n",
    "\n",
    "axs[0].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_stats.index = ct_stats['GEOID']\n",
    "ct_stats = ct_stats.drop('Unnamed: 0', axis=1)\n",
    "ct_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_stats['#chips'] = df.groupby('GEOID').size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_stats.describe().T.to_csv('cencus_tract_stats.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each column of interest\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "mean_total_population = ct_stats['TotalPopulation'].mean()\n",
    "std_total_population = ct_stats['TotalPopulation'].std()\n",
    "\n",
    "mean_area = ct_stats['area'].mean()\n",
    "std_area = ct_stats['area'].std()\n",
    "\n",
    "mean_obesity_rate = ct_stats['OBESITY_CrudePrev'].mean()\n",
    "std_obesity_rate = ct_stats['OBESITY_CrudePrev'].std()\n",
    "\n",
    "# Plotting the histograms and annotating them with mean and standard deviation\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "# Histogram for Total Population\n",
    "ct_stats['TotalPopulation'].hist(ax=axs[0], color='C1')\n",
    "axs[0].set_title('Total Population')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].annotate(f'Mean: {mean_total_population:.1f}\\nSD: {std_total_population:.1f}', \n",
    "                xy=(0.35, 0.83), xycoords='axes fraction', \n",
    "                bbox=dict(boxstyle=\"round\", fc=\"white\"))\n",
    "\n",
    "# Histogram for Area\n",
    "ct_stats['area'].hist(ax=axs[1], color='C1')\n",
    "axs[1].set_title('Area')\n",
    "axs[1].annotate(f'Mean: {mean_area:.1f}\\nSD: {std_area:.1f}', \n",
    "                xy=(0.35, 0.83), xycoords='axes fraction', \n",
    "                bbox=dict(boxstyle=\"round\", fc=\"white\"))\n",
    "\n",
    "ct_stats['OBESITY_CrudePrev'].hist(ax=axs[2], color='C1')\n",
    "axs[2].set_title('Obesity Rate [%]')\n",
    "axs[2].annotate(f'Mean: {mean_obesity_rate:.1f}\\nSD: {std_obesity_rate:.1f}', \n",
    "                xy=(0.05, 0.83), xycoords='axes fraction', \n",
    "                bbox=dict(boxstyle=\"round\", fc=\"white\"))\n",
    "\n",
    "fig.savefig('ct_stats.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average = df.dropna().drop(['Unnamed: 0', 'geometry', 'file', 'jp2', 'image_coords'], axis=1).groupby('index_right').mean()\n",
    "df_average = df_average.reset_index()\n",
    "df_obs_2 = df_average[[\"GEOID\"]]\n",
    "df_obs_2[\"GEOID\"] = df_obs_2[\"GEOID\"].astype('int64')\n",
    "df_obs_2.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_obs_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ar_pop = pd.merge(df_obs_2, obs_area, on='GEOID', how='left')\n",
    "obs_ar_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ar_pop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ar_pop.to_csv(\"obs_area_pop_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average.to_csv('df_average.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of column names to drop\n",
    "columns_to_drop = [f'feature_{i}' for i in range(0, 2048)]\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "df_average.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for the Average Features Vectors for Each Census Tract (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = LinearRegression()\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = Ridge()\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "# file_path = 'joined_gdf.csv'  # Replace it with your file path\n",
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.6].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X = X_selected\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Prepare a DataFrame to store the metrics for each fold\n",
    "metrics_df = pd.DataFrame(columns=['Fold', 'MSE', 'R2', 'Adjusted R2'])\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    # model = LinearRegression()\n",
    "    # Why use ridge regression: Better for situations with multicollinearity, when preventing \n",
    "    # overfitting is important.\n",
    "\n",
    "    model = Ridge(alpha=1.0)  # You can adjust alpha to fine-tune the regularization strength\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Adjusted R2\n",
    "    n = X_test.shape[0]  # Number of observations in the test set\n",
    "    p = 1 #X_test.shape[1]  # Number of features\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "    # Append to DataFrame\n",
    "    print(mse)\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "        'Fold': [fold], 'MSE': [mse], 'R2': [r2], 'Adjusted R2': [adj_r2]})])\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "    plt.xlabel('Actual Obesity Rates')\n",
    "    plt.ylabel('Predicted Obesity Rates')\n",
    "    plt.title(f'Actual vs Predicted Obesity Rates fold {fold}')\n",
    "    plt.show()\n",
    "mean_row = metrics_df.mean()\n",
    "mean_row['Fold'] = 'Mean'\n",
    "mean_row = pd.DataFrame(mean_row).T\n",
    "# Display the table\n",
    "pd.concat([metrics_df, mean_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mean_row).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Linear Regression for Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = Ridge()\n",
    "model_sel.fit(X_selected, y)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_selected)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y, y_pred_sel)\n",
    "r2_sel = r2_score(y, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_selected.shape[0]  # Number of observations\n",
    "p = 1  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y.max()], [y.min(), y.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')#[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = Ridge()\n",
    "model_sel.fit(X_selected, y)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_selected)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y, y_pred_sel)\n",
    "r2_sel = r2_score(y, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "# n = X_selected.shape[0]  # Number of observations\n",
    "# p = X_selected.shape[1]  # Number of predictions\n",
    "# adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "n = X_selected.shape[0]  # Number of observations\n",
    "p = 1#X_test.shape[1]  # Number of features\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2)\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y.max()], [y.min(), y.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data[selected_features]  # Use only selected features\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = Ridge()\n",
    "model_sel.fit(X_selected, y)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_selected)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y, y_pred_sel)\n",
    "r2_sel = r2_score(y, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_selected.shape[0]  # Number of observations\n",
    "# p = X_selected.shape[1]  # Number of selected features\n",
    "# adj_r2_sel = 1 - (1 - r2_sel) * (n - 1) / (n - p - 1)\n",
    "\n",
    "p = 1 #X_test.shape[1]  # Number of features\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, y_pred_sel, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--r')\n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming y is your actual values and y_pred_sel is your predicted values from the model\n",
    "r2_sel = r2_score(y, y_pred_sel)\n",
    "\n",
    "print(\"R-squared:\", r2_sel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for the Average Features Vectors for Each Census Tract (Selected Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_average\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^feature_')[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = LinearRegression()\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red\">The Area Weighted Average for Each Census Tract</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.wkt import loads\n",
    "\n",
    "# Convert the 'geometry' column from WKT format to Shapely geometries\n",
    "gdf = gpd.GeoDataFrame(df, geometry=df['geometry'].apply(loads)) \n",
    "\n",
    "# Calculate the area for each row\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "\n",
    "# Add weighted features\n",
    "gdf = pd.concat([gdf, gdf.iloc[:, 2:2+2048].multiply(gdf.area, axis=0).add_prefix('weighted_')], axis=1)\n",
    "\n",
    "# Get the obesity rate per census tract/GEOID\n",
    "obesity_rates_per_census_tract = gdf[['GEOID', 'OBESITY_CrudePrev']].groupby('GEOID').mean()\n",
    "\n",
    "# Aggregate weighted features based on GEOID\n",
    "columns = ['GEOID', 'area'] + [f'weighted_feature_{i}' for i in range(2048)]\n",
    "weighted_features_per_census_tract = gdf[columns].groupby('GEOID').sum()\n",
    "weighted_features_per_census_tract = weighted_features_per_census_tract.divide(\n",
    "    weighted_features_per_census_tract.area, axis=0)\n",
    "weighted_features_per_census_tract = weighted_features_per_census_tract.drop('area', axis=1)\n",
    "\n",
    "# Add obesity_rate\n",
    "weighted_features_per_census_tract = pd.concat([\n",
    "    obesity_rates_per_census_tract, weighted_features_per_census_tract], axis=1)\n",
    "weighted_features_per_census_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weighted_features_per_census_tract.to_csv('weighted_features_per_census_tract.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_features_per_census_tract = pd.read_csv('weighted_features_per_census_tract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = weighted_features_per_census_tract\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^weighted_feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^weighted_feature_')[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = LinearRegression()\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = 1 #X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "# print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = weighted_features_per_census_tract\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^weighted_feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.2].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^weighted_feature_')[selected_features]\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_sel, X_test_sel, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and fitting the linear regression model on the selected features\n",
    "model_sel = RandomForestRegressor()\n",
    "model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_sel = model_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluating the model on selected features\n",
    "mse_sel = mean_squared_error(y_test, y_pred_sel)\n",
    "r2_sel = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "# Calculate the adjusted R-squared\n",
    "n = X_test_sel.shape[0]  # Number of observations\n",
    "p = 1 #X_test_sel.shape[1]  # Number of predictions\n",
    "adj_r2_sel = 1 - (1-r2_sel) * (n-1) / (n-p-1)\n",
    "\n",
    "# print(\"Selected Features:\", selected_features)\n",
    "print(\"Mean Squared Error:\", mse_sel)\n",
    "print(\"R-squared:\", r2_sel)\n",
    "print(\"Adjusted R-squared:\", adj_r2_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_sel are already defined as our test target values and model predictions respectively\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_sel, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "plt.xlabel('Actual Obesity Rates')\n",
    "plt.ylabel('Predicted Obesity Rates')\n",
    "plt.title('Actual vs Predicted Obesity Rates')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted average 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# file_path = 'joined_gdf.csv'  # Replace it with your file path\n",
    "data = weighted_features_per_census_tract\n",
    "\n",
    "# Compute the correlation of all features with the target variable\n",
    "correlation_with_target = data.filter(regex='^weighted_feature_').apply(lambda x: x.corr(data['OBESITY_CrudePrev']))\n",
    "\n",
    "# Select features with higher correlation (both positive and negative)\n",
    "selected_features = correlation_with_target[correlation_with_target.abs() > 0.6].index.tolist()\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X_selected = data.filter(regex='^weighted_feature_')#[selected_features]\n",
    "\n",
    "\n",
    "# Preparing the selected features and target variable for the model\n",
    "X = X_selected\n",
    "y = data['OBESITY_CrudePrev']\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Prepare a DataFrame to store the metrics for each fold\n",
    "metrics_df = pd.DataFrame(columns=['Fold', 'MSE', 'R2', 'Adjusted R2'])\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(12, 6), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    # model = LinearRegression()\n",
    "    # Why use ridge regression: Better for situations with multicollinearity, when preventing \n",
    "    # overfitting is important.\n",
    "\n",
    "    model = Ridge(alpha=1.0)  # You can adjust alpha to fine-tune the regularization strength\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Adjusted R2\n",
    "    n = X_test.shape[0]  # Number of observations in the test set\n",
    "    p = 1 #X_test.shape[1]  # Number of features\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "    # Append to DataFrame\n",
    "    print(mse)\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "        'Fold': [fold], 'MSE': [mse], 'R2': [r2], 'Adjusted R2': [adj_r2]})])\n",
    "\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    axs[fold].scatter(y_test, y_pred, alpha=0.5)  # Scatter plot of actual vs predicted values\n",
    "    axs[fold].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r') \n",
    "    if fold in [0, 5]:\n",
    "        axs[fold].set_ylabel('Predicted Obesity Rates')\n",
    "    if fold > 4:\n",
    "        axs[fold].set_xlabel('Actual Obesity Rates')\n",
    "    axs[fold].set_title(f'Fold {fold + 1}')\n",
    "\n",
    "plt.suptitle('Actual vs Predicted Obesity Rates (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "mean_row = metrics_df.mean()\n",
    "mean_row['Fold'] = 'Mean'\n",
    "mean_row = pd.DataFrame(mean_row).T\n",
    "# Display the table\n",
    "pd.concat([metrics_df, mean_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/data/JAMIA/final-original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_geometry = df.merge(data, on='GEOID').drop(['OBESITY_CrudePrev_x', 'Unnamed: 0'], axis=1)\n",
    "features_and_geometry.to_csv('features_and_geometry.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_geometry = pd.read_csv('features_and_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [f'weighted_feature_{i}' for i in range(2048)]\n",
    "x = features_and_geometry[features_columns].to_numpy()\n",
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_geometry['pred_obesity'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obesity_gdf = gpd.GeoDataFrame(features_and_geometry, geometry=features_and_geometry['geometry'].apply(loads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=axs[0], legend=True, vmin=0, vmax=2.5)\n",
    "#pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Stoddard'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "#pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Pemiscot'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 1112')\n",
    "axs[0].set_title('Missouri State')\n",
    "#axs[1].set_title('Stoddard County')\n",
    "#axs[2].set_title('Pemiscot County') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming pred_obesity_gdf is preloaded with the necessary data\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Adjusted for a single plot\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=ax, legend=True, vmin=0, vmax=2.5)\n",
    "fig.suptitle('Feature 1112')\n",
    "ax.set_title('Missouri State')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming pred_obesity_gdf is preloaded with the necessary data\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_95']), max(pred_obesity_gdf['weighted_feature_95'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Adjusted for a single plot\n",
    "pred_obesity_gdf.plot(column='weighted_feature_95', cmap='jet', ax=ax, legend=True, vmin=0, vmax=2.5)\n",
    "fig.suptitle('Feature 95')\n",
    "ax.set_title('Missouri State')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming pred_obesity_gdf is preloaded with the necessary data\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1314']), max(pred_obesity_gdf['weighted_feature_1314'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Adjusted for a single plot\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1314', cmap='jet', ax=ax, legend=True, vmin=0, vmax=2)\n",
    "fig.suptitle('Feature 1314')\n",
    "ax.set_title('Missouri State')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming pred_obesity_gdf is preloaded with the necessary data\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1314']), max(pred_obesity_gdf['weighted_feature_1314'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Adjusted for a single plot\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1314', cmap='jet', ax=ax, legend=True, vmin=0, vmax=1)\n",
    "fig.suptitle('Feature 1314')\n",
    "ax.set_title('Missouri State')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=axs[0], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Stoddard'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Pemiscot'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 1112')\n",
    "axs[0].set_title('Missouri State')\n",
    "axs[1].set_title('Stoddard County')\n",
    "axs[2].set_title('Pemiscot County') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=axs[0], legend=True, vmin=0, vmax=2.5)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Stoddard'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Pemiscot'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 1112')\n",
    "axs[0].set_title('Missouri State')\n",
    "axs[1].set_title('Stoddard County')\n",
    "axs[2].set_title('Pemiscot County') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_95']), max(pred_obesity_gdf['weighted_feature_95'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_95', cmap='jet', ax=axs[0], legend=True, vmin=0, vmax=2.5)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Stoddard'].plot(column='weighted_feature_95', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Pemiscot'].plot(column='weighted_feature_95', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 95')\n",
    "axs[0].set_title('Missouri State')\n",
    "axs[1].set_title('Stoddard County')\n",
    "axs[2].set_title('Pemiscot County') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=axs[0], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Reynolds'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Audrain'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 1112')\n",
    "axs[0].set_title('Missouri State')\n",
    "axs[1].set_title('Reynolds County')\n",
    "axs[2].set_title('Audrain County') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_95']), max(pred_obesity_gdf['weighted_feature_95'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_95', cmap='jet', ax=axs[0], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Reynolds'].plot(column='weighted_feature_95', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Audrain'].plot(column='weighted_feature_95', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 95')\n",
    "axs[0].set_title('Missouri State')\n",
    "axs[1].set_title('Reynolds County')\n",
    "axs[2].set_title('Audrain County') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=axs[0], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'St. Louis'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Jackson'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 1112')\n",
    "axs[0].set_title('Missouri State Feature 1112')\n",
    "axs[1].set_title('St. Louis County')\n",
    "axs[2].set_title('Jackson County') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "minv, maxv = min(pred_obesity_gdf['weighted_feature_1112']), max(pred_obesity_gdf['weighted_feature_1112'])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "pred_obesity_gdf.plot(column='weighted_feature_1112', cmap='jet', ax=axs[0], legend=True, vmin=0, vmax=2.5)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'St. Louis'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[1], vmin=minv, vmax=maxv)\n",
    "pred_obesity_gdf[pred_obesity_gdf['CountyName'] == 'Jackson'].plot(column='weighted_feature_1112', cmap='jet', ax=axs[2], vmin=minv, vmax=maxv)\n",
    "fig.suptitle('Feature 1112')\n",
    "axs[0].set_title('Missouri State Feature 1112')\n",
    "axs[1].set_title('St. Louis County')\n",
    "axs[2].set_title('Jackson County') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obesity_gdf[['weighted_feature_1112', 'weighted_feature_95', 'weighted_feature_1314', 'weighted_feature_767']].hist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obesity_gdf[['weighted_feature_239', 'weighted_feature_1253', 'weighted_feature_895']].hist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obesity_gdf[['weighted_feature_1126', 'weighted_feature_338', 'weighted_feature_668']].hist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['weighted_feature_1112', 'weighted_feature_95', 'weighted_feature_1314', 'weighted_feature_767', 'weighted_feature_239', 'weighted_feature_1253', 'weighted_feature_895', 'weighted_feature_1126', 'weighted_feature_338', 'weighted_feature_668']\n",
    "plt.matshow(pred_obesity_gdf[columns].corr())\n",
    "plt.xticks(np.arange(len(columns)), columns)\n",
    "plt.yticks(np.arange(len(columns)), columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['weighted_feature_1112', 'weighted_feature_95', 'weighted_feature_1314', 'weighted_feature_767', 'weighted_feature_239']\n",
    "plt.matshow(pred_obesity_gdf[columns].corr())\n",
    "plt.xticks(np.arange(len(columns)), columns)\n",
    "plt.yticks(np.arange(len(columns)), columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['weighted_feature_1253', 'weighted_feature_895', 'weighted_feature_1126', 'weighted_feature_338', 'weighted_feature_668']\n",
    "plt.matshow(pred_obesity_gdf[columns].corr())\n",
    "plt.xticks(np.arange(len(columns)), columns)\n",
    "plt.yticks(np.arange(len(columns)), columns)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obesity_gdf['error'] = np.sqrt((pred_obesity_gdf['OBESITY_CrudePrev_y'] - pred_obesity_gdf['pred_obesity']) ** 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obesity_gdf['signed_error'] = pred_obesity_gdf['OBESITY_CrudePrev_y'] - pred_obesity_gdf['pred_obesity'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 3))\n",
    "pred_obesity_gdf.plot(column='OBESITY_CrudePrev_y', ax=axs[0], cmap=\"Reds\", legend=True, vmin=25, vmax=55)\n",
    "pred_obesity_gdf.plot(column='pred_obesity', ax=axs[1], cmap=\"Reds\", legend=True, vmin=25, vmax=55)\n",
    "pred_obesity_gdf.plot(column='error', ax=axs[2], cmap=\"jet\", legend=True, vmax=4)\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# legend = axs[0].get_legend()\n",
    "# legend.set_title('Obesity per 100k')\n",
    "axs[0].set_title(label='Actual Obesity Rate (%)')\n",
    "axs[1].set_title(label='Predicted Obesity Rate (%)')\n",
    "axs[2].set_title(label='RMSE')\n",
    "\n",
    "# ax.set_title('Obesity Rates per Census Tract Polygon') # Census Tract-wise Obesity Rates in Missouri \n",
    "# ax.set_title('Census Tract-wise Obesity Rates in Missouri (2022)') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 3))\n",
    "pred_obesity_gdf.plot(column='OBESITY_CrudePrev_y', ax=axs[0], cmap=\"Reds\", legend=True, vmin=25, vmax=55)\n",
    "pred_obesity_gdf.plot(column='pred_obesity', ax=axs[1], cmap=\"Reds\", legend=True, vmin=25, vmax=55)\n",
    "pred_obesity_gdf.plot(column='error', ax=axs[2], cmap=\"jet\", legend=True, vmax=4)\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# legend = axs[0].get_legend()\n",
    "# legend.set_title('Obesity per 100k')\n",
    "axs[0].set_title(label='Actual Obesity Rate (%)')\n",
    "axs[1].set_title(label='Predicted Obesity Rate (%)')\n",
    "axs[2].set_title(label='RMSE')\n",
    "\n",
    "# ax.set_title('Obesity Rates per Census Tract Polygon') # Census Tract-wise Obesity Rates in Missouri \n",
    "# ax.set_title('Census Tract-wise Obesity Rates in Missouri (2022)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_error_df = pred_obesity_gdf.sort_values('error').head(10).drop(['geometry', *[f'weighted_feature_{i}' for i in range(2048)]], axis=1)\n",
    "highest_error_df = pred_obesity_gdf.sort_values('error').tail(10).drop(['geometry', *[f'weighted_feature_{i}' for i in range(2048)]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(18, 3))\n",
    "pred_obesity_gdf.plot(column='OBESITY_CrudePrev_y', ax=axs[0], cmap=\"Reds\", legend=True, vmin=25, vmax=55)\n",
    "pred_obesity_gdf.plot(column='pred_obesity', ax=axs[1], cmap=\"Reds\", legend=True, vmin=25, vmax=55)\n",
    "pred_obesity_gdf.plot(column='error', ax=axs[2], cmap=\"jet\", legend=True, vmin=0, vmax=4)\n",
    "pred_obesity_gdf.plot(ax=axs[3], alpha=0.1)\n",
    "pred_obesity_gdf[pred_obesity_gdf['error'] > 2.5].plot(column='error', ax=axs[3], cmap=\"jet\", legend=True, vmin=0, vmax=4)\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "\n",
    "# legend = axs[0].get_legend()\n",
    "# legend.set_title('Obesity per 100k')\n",
    "axs[0].set_title(label='Actual Obesity Rate (%)')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "axs[1].set_title(label='Predicted Obesity Rate (%)')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "axs[2].set_title(label='Absolute RMSE (% Population)')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "axs[3].set_title(label='Absolute RMSE (% Population)') \n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "\n",
    "# ax.set_title('Obesity Rates per Census Tract Polygon') # Census Tract-wise Obesity Rates in Missouri \n",
    "# ax.set_title('Census Tract-wise Obesity Rates in Missouri (2022)') \n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Longitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_error_df = pred_obesity_gdf.sort_values('error').head(10).drop(['geometry', 'TractFIPS', *[f'weighted_feature_{i}' for i in range(2048)]], axis=1)\n",
    "highest_error_df = pred_obesity_gdf.sort_values('error').tail(10).drop(['geometry', 'TractFIPS', *[f'weighted_feature_{i}' for i in range(2048)]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 5))\n",
    "errors = pred_obesity_gdf['signed_error'].sort_values().reset_index()\n",
    "errors['index'] = np.arange(len(errors))\n",
    "errors_pos = errors[errors['signed_error'] >= 0]\n",
    "errors_neg = errors[errors['signed_error'] < 0]\n",
    "plt.plot(errors_pos['signed_error'], errors_pos['index'], color='green')\n",
    "plt.plot(errors_neg['signed_error'], errors_neg['index'], color='red')\n",
    "plt.yticks([])\n",
    "plt.grid(True)\n",
    "# plt.plot(np.zeros(len(errors)), np.arange(len(errors)), c='k', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3.5, 5))\n",
    "errors = pred_obesity_gdf['signed_error'].sort_values().reset_index()\n",
    "errors['index'] = np.arange(len(errors))\n",
    "errors_pos = errors[errors['signed_error'] >= 0]\n",
    "errors_neg = errors[errors['signed_error'] < 0]\n",
    "plt.plot(errors_pos['signed_error'], errors_pos['index'], color='green')\n",
    "plt.plot(errors_neg['signed_error'], errors_neg['index'], color='red')\n",
    "plt.yticks([])\n",
    "plt.grid(True)\n",
    "plt.xlabel('Signed Error [%]')\n",
    "plt.ylabel('Census Tract [Ranked by Error]')\n",
    "# plt.plot(np.zeros(len(errors)), np.arange(len(errors)), c='k', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
